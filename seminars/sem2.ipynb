{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb25c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random \n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba78bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/grimm/.cache/kagglehub/datasets/ronakp004/autism-spectrum-detection-from-kaggle-zenodo/versions/11\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ronakp004/autism-spectrum-detection-from-kaggle-zenodo\")\n",
    "PATH_TO_DATASET = path + '/Cleaned Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9e91388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set seed\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05132caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "IMAGE_SIZE = 224\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16050166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision.io import decode_image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class AutismDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, transform=None, label_encoder=None, fit_label_encoder=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        \n",
    "        if label_encoder is None:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            if fit_label_encoder:\n",
    "                self.labels = self.label_encoder.fit_transform(self.data['labels'])\n",
    "            else:\n",
    "                raise ValueError(\"Must provide label_encoder or set fit_label_encoder=True\")\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "            self.labels = self.label_encoder.transform(self.data['labels'])\n",
    "        \n",
    "        self.image_paths = self.data['image_path'].tolist()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Load image\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            image = Image.new('RGB', (IMAGE_SIZE, IMAGE_SIZE), color='black')\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "897d6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conservative augmentation for face recognition models\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomCrop((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),  # Reduced probability\n",
    "    transforms.ColorJitter(brightness=0.05, contrast=0.05, saturation=0.05, hue=0.02),  # Very gentle\n",
    "    transforms.RandomRotation(degrees=3),  # Minimal rotation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794f275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AutismDataset(csv_file=os.path.join(PATH_TO_DATASET, 'cleaned_train.csv'),\n",
    "                               transform=train_transform,\n",
    "                               fit_label_encoder=True)\n",
    "\n",
    "val_dataset = AutismDataset(csv_file=os.path.join(PATH_TO_DATASET, 'cleaned_valid.csv'),\n",
    "                             transform=val_test_transform,\n",
    "                             label_encoder=train_dataset.label_encoder)\n",
    "test_dataset = AutismDataset(csv_file=os.path.join(PATH_TO_DATASET, 'cleaned_test.csv'),\n",
    "                              transform=val_test_transform,\n",
    "                              label_encoder=train_dataset.label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1662b42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AutismDataset at 0x7a3c395e6800>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a028345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "\n",
    "writer.add_image('images', grid, 0)\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89dc04a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for n_iter in range(100):\n",
    "    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb51599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/grimm/Documents/Projects/Python/HSSE-CV/seminars'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HSSE-CV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
